train:
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  effective_batch_size: 256
  training_steps: 1000000
  seed: 42
  lr: 0.02
  lr_end: null
  ratio_warmup: null
  warmup_steps: 10000
  label_smoothing: 0.0
  logging_steps: 10
  save_steps: 50
  precision: "bf16"
  scheduler: "t5"
  optimizer: "adamw"

model:
  d_model: 768
  d_state: 256 
  d_ff: 2048
  num_layers: 12
  num_heads: 12        # if you still mix Mamba with attention
  dropout_rate: 0.0
  max_length: 4096

  # Mamba-specific
  d_conv: 4
  expand: 2
  dt_rank: "auto"
  dt_min: 0.001
  dt_max: 0.1
  dt_init: "random"
  dt_scale: 1.0
  dt_init_floor: 1e-4
  conv_bias: true
  bias: false
  use_fast_path: true

data:
  num_workers: 8